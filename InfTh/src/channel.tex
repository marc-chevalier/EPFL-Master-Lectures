Discrete-time channel, discrete valued input/output.

$x_1,x_2\ldots \ra y_1, y_2\ldots$

$x_i \in \X$, $y_i \in \Y$, $\card{X} < \infty$, $\card{Y} < \infty$

Channel will be described by 

\[
    \prob{\underbrace{Y_i = y}_{\substack{\text{Current}\\\text{output}}} | \underbrace{X_i=x_i}_{\substack{\text{Current}\\\text{input}}}, \underbrace{\ldots, X_1 = x_1,Y_{i-1}=y_{i-1},\ldots, Y_1=y_1}_{\text{Events in the past}}}
\]

A channel is said to be memoryless if
\[
    \prob{Y_i = y | X_i=x_i, \ldots, X_1 = x_1,Y_{i-1}=y_{i-1},\ldots, Y_1=y_1} = \prob{Y_i = y | X_i=x_i} = \PP_i(y\vert x_i)
\]

The channel is a \textsc{Markov} chain.

There is a sequence of events:
\begin{enumerate}
    \item $x_1$ is produced $\prob{x_1}$
    \item $y_1$ is produced $\sim \prob{y_1 \vert x_1}$
    \item $x_2$ is produced $\prob{x_2\vert x_1 (y_1)}$
    \item $y_1$ is produced $\sim \prob{y_2 \vert x_2, x_1, y_1}$
\end{enumerate}

We say that the transmission is performed without feedback if
\[
    \prob{x_i \vert x_{i-1} ldots x_1 y_{i-1}\ldots y_1} = \prob{x_i \vert x_{i-1}\ldots x_1}
\]

\begin{theorem}
    If a memoryless channel is used without feedback, then
    \[
        \prob{y_1\ldots y_n\vert x_1\ldots x_n} = \prob{y_1 \vert x_1} \cdots \prob{y_n \vert x_n}
    \]
\end{theorem}
\begin{proof}
    \[
        \begin{aligned}
            \prob{y_1\ldots y_n\vert x_1\ldots x_n} &= \prob{y_1 \vert x_1} \cdots \prob{y_n \vert x_n}\\
            &= \prob{x_1} \prob{y_1\vert x_1} \prob{x_2\vert x_1 y_1}\prob{y_2\vert y_1, x_1, x_2} \ldots\\
            &= \prob{x_1} \prob{y_1\vert x_1} \prob{x_2\vert x_1}\prob{y_2\vert y_1, x_1, x_2} \ldots\\
            &= \prob{x_1} \prob{y_1\vert x_1} \prob{x_2\vert x_1}\prob{y_2\vert x_2} \ldots\\
            &= \prob{x_1\ldots x_n} \prod\limits_{i=1}^n p(y_i \vert x_i)
        \end{aligned}
    \]
    Thus
    \[
        \prob{y_1\ldots y_n \vert x_1\vert x_n} = \prod\limits_{i=1}^n p(y_i \vert x_i)
    \]
\end{proof}

If the source produces a letter each $\tau_S$ seconds (rate of $\rho_S = \frac{1}{\tau_S}$ letters/s) channel accepts input symbols every $\tau_C$ second (rate of $\rho_C = \frac{1}{\tau_C}$), we better have $\tau_S L = \tau_C n$ (where $L$ letters of the sources are encoded into $n$ symbols).

\begin{proposition}
    For a memoryless channel used without feedback 
    \[
        I(X_1\ldots X_n , Y_1 \ldots Y_n) \leqslant \sum\limits_{i=1}^n I(X_i, Y_i)
    \]
\end{proposition}
\begin{proof}
    \[
        \begin{aligned}
            I(X_1\ldots X_n , Y_1 \ldots Y_n) &= \underbrace{H(Y_1\ldots Y_n)}_{\leqslant \sum\limits_{i=1}^n H(Y_i)} - \underbrace{H(Y_1 \ldots Y_n \vert X_1 \ldots X_n)}_{ \begin{aligned}&= \esp{\log\frac{1}{\prob{Y_1\ldots Y_n\vert X_1\ldots X_n}}}\\&=\esp{\log\frac{1}{\prod\limits_{i=1}^n \prob{Y_i\vert X_i}}}\\&=\sum\limits_{i=1}^n\underbrace{\esp{\log\frac{1}{\prob{Y_i\vert X_i}}}}_{H(Y_i\vert X_i}\end{aligned}}\\
            &\leqslant \sum\limits_{i=1}^n H(Y_i) - H(Y_i \vert X_i)\\
            &=\sum\limits_{i=1}^nI(X_i,Y_i)
        \end{aligned}            
    \]
\end{proof}

\begin{definition}
    Given a discrete, memoryless channel $\set{p(y\vert x)}$, define its capacity as
    \[
        C := \max\limits_{p(x)} I(X,Y)
    \]
\end{definition}

\[
    \text{Source} \ra U_1\ldots U_L \overset{\text{Code}}{\longra} X_1\ldots X_n\overset{\text{Channel}}{\longra} Y_1\ldots Y_n \overset{\text{Decode}}{\longra} V_1\ldots V_L
\]

ie $(U_1\ldots U_l) \--- (X_1\ldots X_n) \--- (Y_1\ldots Y_n) \--- (V_1\ldots V_L)$

\begin{theorem}
    Suppose $U_1U_2\ldots$ is a stationary source with entropy rate $H$, and we have the diagram above then
    \[
        \frac{1}{L} H(U_1\ldots U_L\vert V_1\ldots V_l) \geqslant H - \frac{n}{L} C = H - \frac{\tau_S}{\tau_C} C
    \]
\end{theorem}
\begin{proof}
    \[
        \begin{aligned}
            \frac{1}{L} H(U_1\ldots U_L \vert V_1 \ldots V_L) &= \frac{1}{L} H(U_1\ldots U_L) - \frac{1}{L} I(U_1\ldots U_L,V_1\ldots V_L)\\
            & \geqslant H - \frac{n}{L} C
        \end{aligned}
    \]
\end{proof}

\begin{proposition}
    \[
        \frac{1}{L} \sum\limits_{i=1}^n H(U_i \vert V_i) \geqslant \frac{1}{L} H(U_1\ldots U_L \vert V_1\ldots V_L)
    \]
\end{proposition}

\begin{theorem}[\textsc{Fano}'s inequality]
    Suppose $U, V$ are random variable with the same support. Let $p=\prob{U\neq V}$ then
    \[
        H(U\vert V) \leqslant h_2(p) + p\log(\lvert \U \rvert - 1)    
    \]
    where $h_2(p) = p \log \frac{1}{p} + (1-p) \log\frac{1}{1-p}$ is the binary entropy function.
\end{theorem}
\begin{proof}
    Let $W = [U\neq V]$. $H(W) = h_2(p)$.
    
    \[
        \begin{aligned}
            H(UW \vert V) &= H(U\vert V) + H(W\vert UV)\\
            &= H(W\vert V) + H(I\vert WV)\\
            &\leqslant H(W) + H(U\vert WV)\\
            &=h_2(p) + \prob{X=0}H(U\vert W=0,V) + \prob{X=1}H(U\vert W=1,V)
        \end{aligned}            
    \]
\end{proof}

\begin{definition}[Errors]
    \[
        \begin{aligned}
            P_{e,i} &:= \prob{U_i \neq V_i}\\
            \overline{P_e} &:= \frac{1}{L} \sum\limits_{i=1}^L \prob{U_i \neq V_i}
        \end{aligned}
    \]
\end{definition}


\begin{theorem}[Bad news]
    No matter how the Encoder and Decoder are designed, we have
    \[
        h_2(p) + \overline{P_e} \log(\lvert\U\rvert - 1)    \geqslant \tau_S\left( \frac{H}{\tau_S} - \frac{c}{\tau_C}\right)
    \]
\end{theorem}
In particular, if $\delta := \frac{H}{\tau_S} - \frac{c}{\tau_C}>0$ then $\overline{P_e}$ is bounded away from 0 by some function of $\delta$, regardless of how the encoder and decoder is designed.

$P_{e,\max} = \max\limits_i \prob{U_i \neq V_i} \geqslant \overline{P_e}$.
\[
    \begin{aligned}
        P_{e,block} &= \prob{(V_1\ldots V_l)\neq(U_1\ldots U_L)}\\
        &=\prob{\bigcup\limits_{i=1}^L\set{U_i\neq V_i}}\\
        &\geqslant P_{e,\max}
    \end{aligned}
\]

\begin{proof}[Proof of the theorem]
    Suppose we have Enc and Dec as
    \[
        \begin{aligned}
            (U_1\ldots U_L) \to &Enc \to (X_1\ldots X_n)\\
            (Y_1\ldots Y_n) \to &Dec \to (V_1\ldots V_L)
        \end{aligned}   
    \]
    
    We already prove $\frac{1}{L}\sum\limits_{i=1}^L H(U_i \vert V_i) \geqslant H-\frac{\tau_S}{\tau_C} C$.
    
    We also have proved $h_2(P_{e,i}) + P_{e,i}\log(\lvert \U\rvert -1) \geqslant H(U_i \vert V_i)$.
    
    So, we will be done is we can show
    \[
        h_2(\overline{P_e}) + \overline{P_e}\log(\lvert \U\rvert -1) \geqslant \frac{1}{L} \sum\limits_{i=1}^L (h_2(P_{e,i}) + P_{e,i} \log(\card{\U} -1)
    \]
    equivalent to
    \[
        h_2\left( \frac{1}{L} \sum\limits_{i=1}^L P_{e,i} \right) \geqslant \frac{1}{L} \sum\limits_{i=1}^L h_2(P_{e,i})
    \]
    \begin{lemma}
        \[
            h_2\left( \frac{1}{L} \sum\limits_{i=1}^L p_i \right) \geqslant \frac{1}{L} \sum\limits_{i=1}^L h_2(p_i)
        \]
    \end{lemma}        
    \begin{proof}
        Let $a_1\ldots A_L$ be binary random variables with $\prob{A_i = 0} = 1-p_i$ and $\prob{A_1 = 1} = p_i$, we have $H(A_i) = h_2(p_i)$. Let $B\in \brackets{1,L}$, independent of $(A_1\ldots A_L)$ and $\prob{B=i} = \frac{1}{L}$. Let $A=A_B$. Then 
        \[
            \begin{aligned}
                \prob{A = 1} &= \sum\limits_{i=1}^L\prob{A_i = 1 \vert B=i}\prob{B=i}\\
                &= \frac{1}{L} \sum\limits_{i=1}^L p_i
            \end{aligned}                    
        \]
        $\Ra H(A) = h_2\left( \frac{1}{L} \sum\limits_{i=1}^L p_i \right)$.
        
        Also $H(A\vert B) = \sum\limits_{i=1}^L \underbrace{H(A\vert B=i)}_{\underbrace{H(A_i\vert B=i)}_{H(A_i)}}\prob{B=i} = \frac{1}{L}\sum\limits_{i=1}^L h_2(p_i)$
    \end{proof}
\end{proof}

Given a source $(U_1U_2\ldots), \tau_S$ and a channel $\set{P(y\vert x)}, \tau_C$ Let us say that (source, channel) is compatible if $\forall \varepsilon > 0,\exists Enc, Dec: \overline{P_e} < \varepsilon$.

The theorem we just proved is equivalent to (source, channel) is compatible $\Ra$ $\frac{H}{\tau_S} \leqslant \frac{C}{\tau_C}$.

We now attempt the other direction ($\La$).

Suppose we insist on modular design.

$U_1U_2\ldots$ (stationary, entropy rate $H$)

The channel encoder and channel decoder we seek are of the following kind.
\[
    f:\set{0,1}^k \to \X^n \equiv f:\brackets{1,2^k} \to \X^n\overset{\sim}{\equiv} f:\brackets{1,M} \to \X^n
\]

\begin{definition}
    \[
        \opname{rate}(f) = \frac{k}{n} \equiv \frac{\log M}{n}
    \]
\end{definition}

$\emptyset : \Y^n \to \set{0,1}^k$

\begin{definition}
    \[
        \prob{error\vert i} = \prob{\emptyset(Y_1,\ldots,Y_n) \neq i \vert (X_1\ldots X_n) = f(i))}    
    \]
    \[
        \overline{error}(f,\emptyset,channel) = \frac{1}{M} \sum\limits_{i=1}^M \prob{error\vert i}    
    \]
    \[
        error_{\max}(f,\emptyset,channel) = \max\limits_{1\leqslant i\leqslant M} \prob{error \vert i}
    \]
\end{definition}

\begin{definition}
    Given a channel $\prob{y\vert x}$ we say that $R$ is an achievable rate if $\forall \varepsilon > 0$, there exists $f,\emptyset$ such that $rate(f) \geqslant R$, $error_{\max}(f,\emptyset, P) \leqslant \varepsilon$.
\end{definition}

\begin{theorem}
    Given a channel $\prob{y\vert x}$ with $C= \max_{p_X} I(X,Y)$ then any $R < C$ is achievable.
\end{theorem}

\begin{theorem}[Good news]
    If $\frac{H}{\tau_S} < \frac{C}{\tau_C}$, can achieve $\overline{P_e} \to 0$.
\end{theorem}


\section{The probabilistic method}

$(\exists P_z : \prob{A(z)} > 0)\Ra(\exists z : A(z))$.

$(\exists P_z : \esp{g(z)} \leqslant \alpha)\Ra(\exists z : g(z) \leqslant \alpha)$.

\paragraph{Random coding}

Let $X_i^{(m)}$ be iid $p(x)$.

Let $T^n(\varepsilon) = \set{(x,y) \left\vert \left\lvert \widehat{P}_{\X,\Y}(x,y) - p(x)p(y\vert x) \right\rvert \leqslant \varepsilon p(x)p(y\vert x) \right.}$

\begin{itemize}
    \item Find all $m$ such that $(X^{(m)}, Y) \in T^n(\varepsilon)$
    \item If only one such message $m$, output $m$
    \item Otherwise, output 0.
\end{itemize}


\begin{lemma}
    \begin{itemize}
        \item if $(X,Y) \overset{idd}{\sim} p(x)p(y\vert x)$, then $\prob{(X,Y) \in T^n(\varepsilon)} \to 1$ as $n\to \infty$.
        \item If $(X, Y) \overset{idd}{\sim} p(x)p(y)$ then $\prob{(X,Y) \in T^n(\varepsilon)} \leqslant 2^{-n(I(X,Y)-\delta(\varepsilon))}$, $\delta(\varepsilon) \underset{\varepsilon \to 0}{\longra} 0$
    \end{itemize}
\end{lemma}
\begin{proof}
    \item
    \begin{itemize}
        \item LLN
        \item $D(p\Vert q) = \sum\limits_z p(z) \log\frac{p(z)}{q(z)}$ so $D(P_{XY} \Vert P_Xp_Y) = I(X,Y)$
    \end{itemize}
\end{proof}

\section{Binary Symmetric Channel}

$p(y\vert x) = \begin{cases}
    1-\delta & x = y\\
    \delta & x \neq y
\end{cases}$ with $\X = \Y = \set{0,1}$.


\[
    \begin{aligned}
        H(Y \vert X=0) &= H(Y \vert X=1)\\
        &= h_2(\delta)\\
        &= -\delta \log \delta - (1-\delta) \log (1-\delta)
    \end{aligned}
\]

So

\[
    \begin{aligned}
        I(X,Y) &= H(Y) - H(Y\vert X)\\
        &= H(Y) - h_2(\delta)\\
        &\leqslant 1 - h_2(\delta)
    \end{aligned}
\]

$C = 1-h_2(\delta)$ bits.

\section{Binary Erase Channel}

$\X = \set{0,1}$, $\Y= \set{0,1,e}$.

$p(y\vert x) = \begin{cases}
    1-\varepsilon & y=x\\
    \varepsilon & y=e
\end{cases}$

$e$ for erasure.

\[
    \begin{aligned}
        H(X \vert Y = 0) &= 0
        H(X \vert Y = 1) &= 0
        H(X \vert Y = e) &= H(X)
    \end{aligned}
\]

\[
    \begin{aligned}
        I(X,Y) = (1-\varepsilon) H(X)\\
        &\leqslant 1-\varepsilon
    \end{aligned}
\]

$C = 1-\varepsilon$ bits.

\section{Convexity and Concavity}

$f : D \to \RR$ is convex if blablabla...

Fix $p(y\vert x)$ consider $I(P_x) = I(X,Y) $. 

$D = \set{(P_1,\ldots,P_k) \left\vert \card{\X} = k, \forall i\in\brackets{i,k}, P_i\geqslant 0, \sum\limits_{i=1}^n P_i = 1\right.}$

\begin{theorem}
    $I(P_X)$ est concave.
\end{theorem}

Maximize $f(P_X)$ such that $P(x)\geqslant 0$, $\sum\limits_xP_X(x) = 1$.

$f(P_x) = f(P_1,\ldots,P_k)$. Suppose some $(P_1,\ldots,P_k)$ "believed optimal".

If so, it must be that $f(P_1,\ldots,P_i+\varepsilon,\ldots, P_j-\varepsilon,\ldots,P_k) - f(P_1,\ldots,P_k) \leqslant 0$.

LHS $= \varepsilon \frac{\partial f}{\partial P_i}-\varepsilon\frac{\partial f}{\partial P_j}+o(\varepsilon)$. Should have $\forall(i,j), P_j > 0 \Ra \frac{\partial f}{\partial P_i}-\frac{\partial f}{\partial P_j} \leqslant 0$.In particular $\forall(i,j), P_i>0\wedge P_j > 0 \Ra \frac{\partial f}{\partial P_i}-\frac{\partial f}{\partial P_j} = 0$


\begin{definition}[\textsc{Karush Kuhn Tucker} conditions (KKT)]
    $(P_1,\ldots, P_k)$ satisfies KKT if $\exists \lambda:$
    \[
        \begin{aligned}
            \forall i, P_i>0 &\Ra \frac{\partial f}{\partial P_i} = \lambda\\
            \forall i, P_i=0 &\Ra \frac{\partial f}{\partial P_i} \leqslant \lambda\\
        \end{aligned}    
    \]
\end{definition}

$\approx$ like $\underset{x}{\opname{argmax}} g(x)$ must have $\nabla g(x) = G$ (Unconstrained).

\begin{theorem}
    Consider $\max\limits_{\substack{(P_1,\ldots,P_n)\\\forall i\in\brackets{1,n}, P_i \geqslant 0\\\sum\limits_{i=1}^n P_i = 1}} f(x)$ concave differentiable.
    
    $p(x)$ optimal $\LRa$ KKT.
\end{theorem}
\begin{proof}
    ($\La$): ie. if $(P_1,\ldots,P_k)$ satisfies KKT, then $\forall q, (\forall i\in\brackets{1,n},Q_i \geqslant 0\wedge \sum\limits_{i=1}^n Q_i = 1) \Ra f(q) \leqslant f(p)$.
    
    \[
        \begin{aligned}
            f(\theta q + (1-\theta)p) &\geqslant (1-\theta)f(p) + \theta f(q)\\
            f(\theta q + (1-\theta)p) - f(p) &\geqslant -\theta f(p) + \theta f(q)\\
            f(q) - f(p) &\leqslant \frac{1}{\theta}\left( f(p+\theta(q-p)) - f(p) \right)\\
            \to f(q) - f(p) &\leqslant \sum\limits_i (q_i - p_i) \frac{\partial f}{\partial P_i}\\
            &\leqslant \sum\limits_i(q_i-p_i)\lambda\\
            &= \lambda \left(\sum\limits_i q_i - \sum\limits_i p_i\right)\\
            &= 0
        \end{aligned}            
    \]
\end{proof}

\begin{theorem}
    \[
        p(x) \in \opname{argmax} I(X,Y) \LRa \exists \lambda: \forall x, \sum\limits_y p(y\vert x) \log \frac{p(y\vert x)}{p(y)} \leqslant \lambda
    \]
    with equality when $p(x) > 0$. If so, then $\lambda = C$
\end{theorem}
$\approx$ each used input reveals the same amount of information.
\begin{proof}
    Use KKT, prove $\frac{\partial I(X,Y)}{\partial P(x)} = \sum\limits_y p(y\vert x) \log \frac{p(y\vert x)}{p(y)}-\log e$. (careful: $p(y)$ depends on $p(y)$). $I(X,Y) = H(Y) - H(Y\vert X)$.

    \bigskip
    
    ($\lambda = C$): sum $p(x) \sum\limits_y p(y\vert x) \log \frac{p(y\vert x)}{p(x)} = \lambda p(x)$ over $x$.    
\end{proof}

\begin{remark}
    For symmetric channels, can "guess" $P_x = \left( \frac{1}{K},\ldots,\frac{1}{K}\right)$ and verify KKT.
\end{remark}

\begin{definition}[Z-channel]
    \[
        \begin{aligned}
            p(0 \to 0) &= 1\\
            p(1 \to 0) &= \delta\\
            p(1 \to 1) &= 1 - \delta
        \end{aligned}
    \]
\end{definition}

\[
    \begin{aligned}
        p_Y(0) &= 1-P_X(1)(1-\delta)\\
        P_Y(1) &= P_X(1)(1-\delta)
    \end{aligned}            
\]

KKT:
\begin{itemize}
    \item $x=0$: $\lambda = \log \frac{1}{P_Y(0)}$
    \item $x=1$: $\lambda = \delta \log \frac{\delta}{P_Y(0)} + (1-\delta) \log \frac{1-\delta}{P_Y(1)}$
\end{itemize}


\[
    \begin{aligned}
        h_2(\delta) &= -\delta \log \delta - (1-\delta) \log(1-\delta)\\
        \lambda &= -h_2(\delta) + \delta \lambda - (1-\delta) \log\left (1-2^{-\lambda}\right )\\
        \lambda &= \log\left( 1-2^{-\frac{h_2(\delta)}{1-\delta}} \right) = C\\
        P_Y(0) &= 2^{-\lambda}\\
        &= \frac{1}{1+2^{\frac{-h_2(\lambda)}{1-\delta}}}\\
        P_Y(1) &= 1- P_Y(0)\\
        P_X(1) &= \frac{P_Y(1)}{1-\delta}\\
        &= \frac{1}{(1-\delta)\left(1+2^{\frac{h_2(\delta)}{1-\delta}}\right)}\\
        P-X(0) &= 1-P_X(1)
    \end{aligned}
\]

\section{Continuous channels}

So far: discrete (memoryless) channels.

But we would to add a Gaussian noise. So, we need a real-valued channel.

Let $(X,Y)$ have probability density functions $f_X, f_Y, f_{XY}, f_{Y\vert X}$ etc..

\[
    \begin{aligned}
        h(X) &= \int f_X(x) \log f_X(x) \dd x\\
        &= \esp{-\log f_X(X)}\\
        h(X\vert Y) &= \esp{-\log f_{X\vert Y} (X\vert Y)}\\
        h(X,Y) &= \esp{-\log f{XY}(X,Y)}\\
        D(f_X \Vert g_X) &= \EE_f\left[\log\frac{f_X(X)}{g_X(X)}\right]\\
        &= \int f_X(x) \log \frac{f_X(x)}{g_X(x)} \dd x\\
        I(X,Y) &= D(f_{XY} \Vert f_X f_Y)\\
        &=\iint f_{XY}(x,y) \log \frac{f_{Y\vert X}(y,x)}{f_Y(y)} \dd x\dd y
    \end{aligned}
\]

\begin{enumerate}
    \item $I(X,Y) = h(X) - h(X\vert Y) = h(Y) = h(Y\vert X)$ \textsc{Bayes} ok
    \item $D(f\Vert g) \geqslant 0$ with equality $\LRa$ $f = g$
    \item $I(X,Y) \geqslant 0$ with equality $\LRa$ $X \perp\hspace{-7pt}\perp Y$
    \item Conditioning reduces $h$: $h(X\vert Y) \leqslant h(X)$
    \item Chain rules: $h(X_1,\ldots, X_n) = h(X_1) + h(X_2 \vert X_1) + \cdots + h(X_n \vert X_1,\ldots,X_{n-1})$
    \item $h(X_1,\ldots,X_n) \leqslant \sum\limits_{i=1}^n h(X_i)$
\end{enumerate}

But

$h(X) \not\geqslant 0$ in general.

\begin{example}
    $X \sim \U\left( \left[ \frac{-a}{2},\frac{a}{2} \right] \right)$.
    
    \[
        \begin{aligned}
            h(X) &= \log_2 a\\
        \end{aligned}
    \]
\end{example}

\begin{proposition}
    \[
        \begin{aligned}
            \forall c \in \RR, &h(X + c) = h(X)\\
            &h(cX) = \log \lvert c \rvert + h(X)
        \end{aligned}
    \]
\end{proposition}

\begin{definition}[Entropy typical set]
    \[
        T^n(\varepsilon) = \set{x \left\vert \left\lvert \frac{1}{n} \log f_X(x_i) - h(X) \right\rvert \leqslant \varepsilon \right.}
    \]
\end{definition}

\begin{proposition}
    Discrete case
    \[
        \card{T^n(\varepsilon)} = 2^{n(H(X) + \delta(\varepsilon))}
    \]

    Continuous case
    \[
        Vol(T^n(\varepsilon)) = 2^{nh(X) + \delta(\varepsilon)}
    \]
    where $\delta \underset{\varepsilon \to 0}{\to} 0$.
\end{proposition}

\begin{proposition}
    \[
        \prob{X \in T^n(\varepsilon)} \to 1
    \]
\end{proposition}

Let $X$ be continuous rv with a continuous $f_X(x)$ and $X_\Delta = \Delta_i$ for $\Delta i \leqslant X < \Delta(i+1)$.

\begin{theorem}
    \[
        \lim\limits_{\Delta \to 0}( H(X_\Delta) + \log \Delta) = h(X)
    \]
\end{theorem}
\begin{proof}
    \[
        \begin{aligned}
            p_i &= \prob{X_\Delta = \Delta_i}\\
            &= \int_{i\Delta}^{(i+1)\Delta} f(x) \dd x\\
            &= f(x_i) \Delta \text{\qquad for some }x_i\in [i\Delta, (i+1)\Delta]
        \end{aligned}
    \]
    
    \[
        \begin{aligned}
            H(X_\Delta) &= - \sum\limits_i p_i\log p_i\\
            &= - \sum\limits_i \Delta f(x_i) \log (\Delta f(x_i))\\
            &= \underbrace{- \sum\limits_i \Delta f(x_i) \log f(x_i)}_{\scriptscriptstyle \begin{aligned}\underset{\Delta \to 0}{\to} &-\int f(x) \log f(x)\dd x \\ &h(X) \end{aligned}} - \underbrace{\underbrace{\sum\limits_i f(x_i) \Delta}_{\int f(x) \dd x = 1} \log \Delta}_{\log \Delta}
        \end{aligned}            
    \]
\end{proof}

\begin{theorem}
    \[
        I(X_\Delta, Y_\Delta) \underset{\Delta\to 0}{\to} I(X,Y)
    \]
\end{theorem}
\begin{proof}
    \[
        \begin{aligned}
            I(X_\Delta, Y) &= H(X_\Delta) - H(X_\Delta \vert Y)\\
            &= (h(X - \log \Delta + o(\Delta)) - (h(X\vert Y) - \log \Delta + o(\Delta))\\
            &= h(X) - h(X\vert Y) + o(\Delta)\\
            &\to I(X,Y)
        \end{aligned}
    \]
    
    So $I(X_\Delta, Y) \to I(X,Y)$. Similarly $I(X, Y_\Delta) \to I(X,Y)$ and $I(X_\Delta, Y_\Delta) \to I(X,Y_\Delta)$. So $I(X_\Delta, Y_\Delta) \to I(X,Y)$.
\end{proof}

\subsection{Gaussian Noise}

$X \sim \N(0,\sigma^2)$.

\[
    f_X(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{\frac{-x^2}{2\sigma^2}}
\]

\[
    \begin{aligned}
        h(X) &= \esp{-\ln f_X(x)}\\
        &= \esp{\frac{1}{2} \ln(2\pi\sigma^2) + \frac{1}{2\sigma^2}X^2}\\
        &= \frac{1}{2}\left( 1+\ln(2\pi\sigma^2) \right)\\
        &= \frac{1}{2} \ln(2\pi e\sigma^2)\text{ NATS}\\
        &= \frac{1}{2} \log(2\pi e\sigma^2) \text{ Bits}
    \end{aligned}
\]

\begin{theorem}[Maximal entropy]
    If $\Var{X} = \sigma^2$, then $h(X) \leqslant \frac{1}{2} \log (2\pi e\sigma^2)$ with equality iff $X\sim \N(\mu,\sigma^2)$ for some $\mu$.
\end{theorem}
\begin{proof}
    Wlog $\esp{X} = 0$.
    
    Let $X$ have pdf $f$ and let $g$ be the $N(0,\sigma^2)$ pdf.
    
    \[
        \begin{aligned}
            \int f(x) \log g(x) \dd x &= \EE_f\left[ c_1 + c_2 X^2 \right]\\
            &= \EE_g\left[ c_1 + c_2 X^2\right]\\
            &= \int g(x) \log g(x) \dd x
        \end{aligned}
    \]
    
    Goal: show $-\int f(x) \log f(x) \dd x\leqslant -\int g(x) \log g(x) \dd x \LRa -\int f(x) \log f(x) \dd x\leqslant -\int f(x) \log g(x) \dd x$. Ie. $\int f(x) \log \frac{f(x)}{g(x)} \geqslant 0$ ie. $\D(f\Vert g) \geqslant 0$.
\end{proof}

\subsection{Vector Extensions}
\begin{itemize}
    \item If $X \in \RR^d$, $X=\N(0, K)$ where $K \in \M_d(\RR)$, then $h(X) = \frac{1}{2}\log \det(2\pi e K)$.
    \item $\opname{Cov}(X)) = K$, then $h(X) \leqslant \frac{1}{2} \log \det (2\pi e K)$.
    \item $h(X+\varepsilon = h(X)$.
    \item $h(AX) = h(X) + \log \lvert\det A \rvert$.
\end{itemize}

\subsection{Additive White Gaussian Noise Channel}

$Y = X + Z$, $Z\sim \N(0,\sigma^2)$. White noise: $Z_i \perp\hspace{-7pt}\perp Z_j$ for $i\neq j$ (ie. memoryless).

\[
    \begin{aligned}
        I(X,Y) &= I(X+Z) - \underbrace{h(X+Z \vert X}_{\scriptscriptstyle\begin{aligned}
                &= h(Z + X)\\&=h(Z)\\&=\frac{1}{2}\log(2\pi e \sigma^2))
        \end{aligned}}
    \end{aligned}
\]

Cost function $b(x)$.

Average constraint $\frac{1}{M} \sum\limits_{n=1}^{M} \frac{1}{n} \sum\limits_{i=1}^n b(x_i^{(u)}) \leqslant P$.

\begin{theorem}
    If $\frac{H}{\tau_S} > \frac{C(P)}{\tau_C}$, any code  with $\sum\limits_u p(u) \frac{1}{n} \sum\limits_{i=1}^nb(x^{(u)}_i) \leqslant P$ has bit error rate $P_e \not\to 0$.
\end{theorem}

\begin{lemma}
    \[
        \sum\limits_{i=1}^n I(X_i, Y_i) \leqslant , C(P)
    \]
\end{lemma}

\subsection{Parallel Gaussian Channel}

Let $x = (X_1,\ldots,X_k)$.

$Z_1 \sim\N(0,\sigma_1^2)$ and  $Z_k \sim\N(0,\sigma_k^2)$

$Y_i = X_i + Z_i$.

$Z_i$ pairwise independent.

Power constraint

$b(x) = b(x_1,\ldots,x_n) = \sum\limits_{i=1}^k x_i^2$

Constrain $\esp{b(X)} \leqslant P$.

$C(P) = \max\limits_{\sum\limits_{i=1}^k \esp{X_i^2}} I(X_1,\ldots,X_k ; Y_1,\ldots, Y_k) = \max\limits_{\substack{P_1,\ldots,P_k\\P_i \geqslant 0\\\sum\limits_{i=1}^k P_i \leqslant P}} \sum\limits_{i=1}^n\frac{1}{2} \log\left(1+\frac{P_i}{\sigma_i^2}\right)$.

Let $\alpha_i=\frac{P_i}{P}$ (proportion of power used in subchannel $i$)

KKT : 
\[
    \begin{aligned}
        \text{Optimal} &\LRa \exists \lambda: \frac{\partial f}{\partial \alpha_i} \leqslant\lambda\\
        &\LRa \exists \lambda : \sigma_i^2+P\alpha_i \geqslant \lambda\\
        &\LRa \exists \lambda \sigma_i^2+P_i \geqslant \lambda \text{ with equality if }P_i > 0
    \end{aligned}
\]
$P_i = \max\set{0,\lambda - \sigma_i^2}$

\section{Build Your Own Encoder/Decoder}

For a while, we will be concerned with the Binary Symmetric Channel  ($\prob{x=y} = p$ et $\prob{x=1-y} = 1-p$).

Code for the BSC is a array $M (=2^{nR}) \times n$ of binary entries.

Given $\overline{y} = (y_1,\ldots,y_n)$ find the codeword (a table row) closet to $\overline{y}$ in \textsc{Hamming} distance.

Why?
\[
    \begin{aligned}
        \prob{\overline{y}\vert \overline{x}} &= \prod\limits_{i=1}^n \underbrace{\prob{y_k \vert x_i}}_{\begin{cases}1-p & y_i = x_i\\p & y_i \neq x_i\end{cases}}\\
        &= (1-p)^n \left( \frac{p}{1-p} \right)^{\card{\set{i}{y_i\neq x_i}}}
        &= \text{decreasing in }d_H(\overline{x},\overline{y})
    \end{aligned}
\]
So
\[
    \text{Maximum likehood} \LRa \text{Minimum distance}
\]

\begin{definition}[\textsc{Hamming} distance]
    \[
        d_H(\overline{x}, \overline{y}) := \card{\set{i\vert y_i\neq x_i}}
    \]
\end{definition}

We immediately see $(n, nR)$ as visible parameters of the code (named $\C$). Let us also add $d = d_H(\C) = \min\limits_{\substack{1\leqslant i\leqslant M\\1\leqslant j < i}} d_H\left(X^{(i)}, X^{(j)}\right)$. as the minimum distance of $\C$ to these parameter.


\begin{theorem}[\textsc{Singleton} bound]
    \[
        M > 2^k \Ra d \geqslant n - k
    \]
    where $M$ is the number of codewords.
\end{theorem}
\begin{proof}
    Given $k$ columns, we can only write $2^k$ different things. So, is we have to write more than $2^k$ codewords, at least 2 will agree on these $k$ columns. So, their \textsc{Hamming} distance is at most $n - k$.
\end{proof}

\begin{corollary}[\textsc{Singleton} bound, other style]
    \[
        M = 2^k \Ra d \geqslant n - k + 1
    \]
    where $M$ is the number of codewords.
\end{corollary}

\begin{theorem}
    The \textsc{Hamming} distance is a distance.
\end{theorem}
\begin{proof}
    Isn't it obvious?
\end{proof}

\begin{corollary}
    Suppose $(x, x') \in \left(\set{0,1}^n\right)^2$ with $d = d_H(x, x')$. Set $r = \floor{ \frac{d-1}{2}}$. Then $\mathcal{B}(x, r) \cap \mathcal{B}(x',r) = \emptyset$.
\end{corollary}
\begin{proof}
    Suppose not, then $\exists \overline{y} : d_H(x,y) \leqslant r \wedge d_H(x',y) \leqslant r$. But $d_H(x,y) \leqslant r \Ra d_H(x',y) \leqslant r \Ra d(x,x') \leqslant 2r < d$. Contradiction.
\end{proof}

Also note $\card{\mathcal{B}(x,r)} = \sum\limits_{k=0}^{\min(n,r)}\binom{n}{k}$.

\begin{theorem}[Sphere packing bound]
    Suppose $\C \subseteq \set{0,1}^n$ is a binary code with $M$ codeword. Then
    \[
        M \sum\limits_{k=0}^{\min\left(n,\ceil{\frac{d-1}{2}}\right)}\binom{n}{k} \leqslant 2^n
    \]
\end{theorem}
\begin{proof}
    By the corollary, the balls $\set{\mathcal{B}(x,r)\vert x \in \C}$ are all disjoint. Then
    \[
        \sum\limits_{x\in \C}\card{\mathcal{B}(x,r)} = \card{\bigcup\limits_{x \in \C} \mathcal{B}(x,r)}
    \]

    But $\bigcup\limits_{x \in \C} \mathcal{B}(x,r) \subseteq \set{0,1}^n$ so
    
    \[
        \card{\bigcup\limits_{x \in \C} \mathcal{B}(x,r)} \leqslant 2^n
    \]
\end{proof}


\begin{theorem}[\textsc{Gilbert-Varshamov} bound]
    Given $n, d$, there exists a code $\C$ with blocklength $n$, $d_{\min} \geqslant d$
    \[
        M \sum\limits_{k=0}^{d-1} \binom{n}{k} \geqslant 2^n
    \]
\end{theorem}
\begin{proof}
    We take a word in $\set{0,1}^n$ and we mark the ball of radius $d-1$. And we redo with an unmarked word until the whole space is marked.
    \[
        \underbrace{M}_{\text{\# of codeword}} \underbrace{\sum\limits_{k=0}^{d-1} \binom{n}{k}}_{\geqslant \text{\# of word marked at each step}} \geqslant \underbrace{2^n}_{\text{total number of words}}
    \]
\end{proof}

\begin{definition}
    A code $\C\subseteq\set{0,1}^M$ is said to be linear if $\forall (x,y)\in\C^2, x+y\in\C$.
\end{definition}

\begin{example}
    $\C = \set{001,111}$ is not linear ($000$ missing).
    
    $\C = \set{001,111,000,110}$ is linear since 
    
    \[
        \begin{aligned}
            \C&=\opname{Vect}_{\ZZ/2\ZZ}((001,111))\\
            &= \set{\left(\begin{matrix}u_1&u_2\end{matrix}\right)\left(\begin{matrix}0&0&1\\1&1&1\end{matrix}\right)\vert (u_1,u_2)\in\set{0,1}^2}\\
            &= \set{\left(\begin{matrix}0&1\\0&1\\1&1\end{matrix}\right)\left(\begin{matrix}u_1\\u_2\end{matrix}\right)\vert (u_1,u_2)\in\set{0,1}^2}
        \end{aligned}
    \]
\end{example}

\begin{proposition}
    If $\C$ is a linear code, then $\card{\C} = 2^k$ for some integer $k$, and there exists a $k\times n$ binary matrix $G$ such that
    \[
        \C = \M_{1,k}(\ZZ/2\ZZ)G    
    \]
\end{proposition}

\begin{proposition}
    If $\C\subseteq \set{0,1}^M$ is a linear code whith $\card{\C} = 2^k$, then there exists a $n\times(n-1)$ matrix $H$ such that $\C = \opname{Ker} H$.
\end{proposition}

$H$ is called the parity-check matrix.

\begin{definition}
    The \textsc{Hamming} weight $w_H(x)$ of a vector $x$ is the sum of its components. Ie. it is the 1-norm.
\end{definition}

\begin{definition}
    Given a linear code $\C$, let $w_{\min}(\C) = \min_{\substack{x\in\C\\x\neq 0}w_H(x)}$.
\end{definition}

\begin{theorem}
    For a linear code $\C$, $w_H(\C) = d_{\min}(\C)$
\end{theorem}
\begin{proof}
    \[
        d_H(x,0) = w_H(x)
    \]
    so
    \[
        x_H(\C) \geqslant d_{min}(C)
    \]
    Also if $x\neq x' \in \C$, $w_H(x+x') = d_H(x,x')$.
\end{proof}

\begin{example}
    Let
    \[
        H = \left(
            \begin{matrix}
                1 & 0 & 0\\
                0 & 1 & 0\\
                0 & 0 & 1\\
                1 & 1 & 0\\
                0 & 1 & 1\\
                1 & 0 & 1\\
                1 & 1 & 1\\
            \end{matrix}
        \right)
    \]
    
    Let $\C = \opname{Ker} H^T$.
    
    \[
        \left(
        \begin{matrix}
        x_1\\x_2\\x_3
        \end{matrix}
        \right) = \left( \begin{matrix}
        1&0&1&1\\
        1&1&0&1\\
        0&1&1&1\\
        \end{matrix} \right) \left( \begin{matrix}
        x_4\\x_5\\x_6\\x_7
        \end{matrix}\right)
    \]
    So $\card{\C} = 16$.
    The spheres of radius 1 around the 16 codewords are disjoint and completely cover $\set{0,1}^7$.
\end{example}

We can generalize to $(2^m-1,2^m-m-1,3)$ \textsc{Hamming} code.














