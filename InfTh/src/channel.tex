Discrete-time channel, discrete valued input/output.

$x_1,x_2\ldots \ra y_1, y_2\ldots$

$x_i \in \X$, $y_i \in \Y$, $\card{X} < \infty$, $\card{Y} < \infty$

Channel will be described by 

\[
    \prob{\underbrace{Y_i = y}_{\substack{\text{Current}\\\text{output}}} | \underbrace{X_i=x_i}_{\substack{\text{Current}\\\text{input}}}, \underbrace{\ldots, X_1 = x_1,Y_{i-1}=y_{i-1},\ldots, Y_1=y_1}_{\text{Events in the past}}}
\]

A channel is said to be memoryless if
\[
    \prob{Y_i = y | X_i=x_i, \ldots, X_1 = x_1,Y_{i-1}=y_{i-1},\ldots, Y_1=y_1} = \prob{Y_i = y | X_i=x_i} = \PP_i(y\vert x_i)
\]

The channel is a \textsc{Markov} chain.

There is a sequence of events:
\begin{enumerate}
    \item $x_1$ is produced $\prob{x_1}$
    \item $y_1$ is produced $\sim \prob{y_1 \vert x_1}$
    \item $x_2$ is produced $\prob{x_2\vert x_1 (y_1)}$
    \item $y_1$ is produced $\sim \prob{y_2 \vert x_2, x_1, y_1}$
\end{enumerate}

We say that the transmission is performed without feedback if
\[
    \prob{x_i \vert x_{i-1} ldots x_1 y_{i-1}\ldots y_1} = \prob{x_i \vert x_{i-1}\ldots x_1}
\]

\begin{theorem}
    If a memoryless channel is used without feedback, then
    \[
        \prob{y_1\ldots y_n\vert x_1\ldots x_n} = \prob{y_1 \vert x_1} \cdots \prob{y_n \vert x_n}
    \]
\end{theorem}
\begin{proof}
    \[
        \begin{aligned}
            \prob{y_1\ldots y_n\vert x_1\ldots x_n} &= \prob{y_1 \vert x_1} \cdots \prob{y_n \vert x_n}\\
            &= \prob{x_1} \prob{y_1\vert x_1} \prob{x_2\vert x_1 y_1}\prob{y_2\vert y_1, x_1, x_2} \ldots\\
            &= \prob{x_1} \prob{y_1\vert x_1} \prob{x_2\vert x_1}\prob{y_2\vert y_1, x_1, x_2} \ldots\\
            &= \prob{x_1} \prob{y_1\vert x_1} \prob{x_2\vert x_1}\prob{y_2\vert x_2} \ldots\\
            &= \prob{x_1\ldots x_n} \prod\limits_{i=1}^n p(y_i \vert x_i)
        \end{aligned}
    \]
    Thus
    \[
        \prob{y_1\ldots y_n \vert x_1\vert x_n} = \prod\limits_{i=1}^n p(y_i \vert x_i)
    \]
\end{proof}

If the source produces a letter each $\tau_S$ seconds (rate of $\rho_S = \frac{1}{\tau_S}$ letters/s) channel accepts input symbols every $\tau_C$ second (rate of $\rho_C = \frac{1}{\tau_C}$), we better have $\tau_S L = \tau_C n$ (where $L$ letters of the sources are encoded into $n$ symbols).

\begin{proposition}
    For a memoryless channel used without feedback 
    \[
        I(X_1\ldots X_n , Y_1 \ldots Y_n) \leqslant \sum\limits_{i=1}^n I(X_i, Y_i)
    \]
\end{proposition}
\begin{proof}
    \[
        \begin{aligned}
            I(X_1\ldots X_n , Y_1 \ldots Y_n) &= \underbrace{H(Y_1\ldots Y_n)}_{\leqslant \sum\limits_{i=1}^n H(Y_i)} - \underbrace{H(Y_1 \ldots Y_n \vert X_1 \ldots X_n)}_{ \begin{aligned}&= \esp{\log\frac{1}{\prob{Y_1\ldots Y_n\vert X_1\ldots X_n}}}\\&=\esp{\log\frac{1}{\prod\limits_{i=1}^n \prob{Y_i\vert X_i}}}\\&=\sum\limits_{i=1}^n\underbrace{\esp{\log\frac{1}{\prob{Y_i\vert X_i}}}}_{H(Y_i\vert X_i}\end{aligned}}\\
            &\leqslant \sum\limits_{i=1}^n H(Y_i) - H(Y_i \vert X_i)\\
            &=\sum\limits_{i=1}^nI(X_i,Y_i)
        \end{aligned}            
    \]
\end{proof}

\begin{definition}
    Given a discrete, memoryless channel $\set{p(y\vert x)}$, define its capacity as
    \[
        C := \max\limits_{p(x)} I(X,Y)
    \]
\end{definition}

\[
    \text{Source} \ra U_1\ldots U_L \overset{\text{Code}}{\longra} X_1\ldots X_n\overset{\text{Channel}}{\longra} Y_1\ldots Y_n \overset{\text{Decode}}{\longra} V_1\ldots V_L
\]

ie $(U_1\ldots U_l) \--- (X_1\ldots X_n) \--- (Y_1\ldots Y_n) \--- (V_1\ldots V_L)$

\begin{theorem}
    Suppose $U_1U_2\ldots$ is a stationary source with entropy rate $H$, and we have the diagram above then
    \[
        \frac{1}{L} H(U_1\ldots U_L\vert V_1\ldots V_l) \geqslant H - \frac{n}{L} C = H - \frac{\tau_S}{\tau_C} C
    \]
\end{theorem}
\begin{proof}
    \[
        \begin{aligned}
            \frac{1}{L} H(U_1\ldots U_L \vert V_1 \ldots V_L) &= \frac{1}{L} H(U_1\ldots U_L) - \frac{1}{L} I(U_1\ldots U_L,V_1\ldots V_L)\\
            & \geqslant H - \frac{n}{L} C
        \end{aligned}
    \]
\end{proof}

\begin{proposition}
    \[
        \frac{1}{L} \sum\limits_{i=1}^n H(U_i \vert V_i) \geqslant \frac{1}{L} H(U_1\ldots U_L \vert V_1\ldots V_L)
    \]
\end{proposition}

\begin{theorem}[\textsc{Fano}'s inequality]
    Suppose $U, V$ are random variable with the same support. Let $p=\prob{U\neq V}$ then
    \[
        H(U\vert V) \leqslant h_2(p) + p\log(\lvert \U \rvert - 1)    
    \]
    where $h_2(p) = p \log \frac{1}{p} + (1-p) \log\frac{1}{1-p}$ is the binary entropy function.
\end{theorem}
\begin{proof}
    Let $W = [U\neq V]$. $H(W) = h_2(p)$.
    
    \[
        \begin{aligned}
            H(UW \vert V) &= H(U\vert V) + H(W\vert UV)\\
            &= H(W\vert V) + H(I\vert WV)\\
            &\leqslant H(W) + H(U\vert WV)\\
            &=h_2(p) + \prob{X=0}H(U\vert W=0,V) + \prob{X=1}H(U\vert W=1,V)
        \end{aligned}            
    \]
\end{proof}

\begin{definition}[Errors]
    \[
        \begin{aligned}
            P_{e,i} &:= \prob{U_i \neq V_i}\\
            \overline{P_e} &:= \frac{1}{L} \sum\limits_{i=1}^L \prob{U_i \neq V_i}
        \end{aligned}
    \]
\end{definition}


\begin{theorem}[Bad news]
    No matter how the Encoder and Decoder are designed, we have
    \[
        h_2(p) + \overline{P_e} \log(\lvert\U\rvert - 1)    \geqslant \tau_S\left( \frac{H}{\tau_S} - \frac{c}{\tau_C}\right)
    \]
\end{theorem}
In particular, if $\delta := \frac{H}{\tau_S} - \frac{c}{\tau_C}>0$ then $\overline{P_e}$ is bounded away from 0 by some function of $\delta$, regardless of how the encoder and decoder is designed.

$P_{e,\max} = \max\limits_i \prob{U_i \neq V_i} \geqslant \overline{P_e}$.
\[
    \begin{aligned}
        P_{e,block} &= \prob{(V_1\ldots V_l)\neq(U_1\ldots U_L)}\\
        &=\prob{\bigcup\limits_{i=1}^L\set{U_i\neq V_i}}\\
        &\geqslant P_{e,\max}
    \end{aligned}
\]

\begin{proof}[Proof of the theorem]
    Suppose we have Enc and Dec as
    \[
        \begin{aligned}
            (U_1\ldots U_L) \to &Enc \to (X_1\ldots X_n)\\
            (Y_1\ldots Y_n) \to &Dec \to (V_1\ldots V_L)
        \end{aligned}   
    \]
    
    We already prove $\frac{1}{L}\sum\limits_{i=1}^L H(U_i \vert V_i) \geqslant H-\frac{\tau_S}{\tau_C} C$.
    
    We also have proved $h_2(P_{e,i}) + P_{e,i}\log(\lvert \U\rvert -1) \geqslant H(U_i \vert V_i)$.
    
    So, we will be done is we can show
    \[
        h_2(\overline{P_e}) + \overline{P_e}\log(\lvert \U\rvert -1) \geqslant \frac{1}{L} \sum\limits_{i=1}^L (h_2(P_{e,i}) + P_{e,i} \log(\card{\U} -1)
    \]
    equivalent to
    \[
        h_2\left( \frac{1}{L} \sum\limits_{i=1}^L P_{e,i} \right) \geqslant \frac{1}{L} \sum\limits_{i=1}^L h_2(P_{e,i})
    \]
    \begin{lemma}
        \[
            h_2\left( \frac{1}{L} \sum\limits_{i=1}^L p_i \right) \geqslant \frac{1}{L} \sum\limits_{i=1}^L h_2(p_i)
        \]
    \end{lemma}        
    \begin{proof}
        Let $a_1\ldots A_L$ be binary random variables with $\prob{A_i = 0} = 1-p_i$ and $\prob{A_1 = 1} = p_i$, we have $H(A_i) = h_2(p_i)$. Let $B\in \brackets{1,L}$, independent of $(A_1\ldots A_L)$ and $\prob{B=i} = \frac{1}{L}$. Let $A=A_B$. Then 
        \[
            \begin{aligned}
                \prob{A = 1} &= \sum\limits_{i=1}^L\prob{A_i = 1 \vert B=i}\prob{B=i}\\
                &= \frac{1}{L} \sum\limits_{i=1}^L p_i
            \end{aligned}                    
        \]
        $\Ra H(A) = h_2\left( \frac{1}{L} \sum\limits_{i=1}^L p_i \right)$.
        
        Also $H(A\vert B) = \sum\limits_{i=1}^L \underbrace{H(A\vert B=i)}_{\underbrace{H(A_i\vert B=i)}_{H(A_i)}}\prob{B=i} = \frac{1}{L}\sum\limits_{i=1}^L h_2(p_i)$
    \end{proof}
\end{proof}

Given a source $(U_1U_2\ldots), \tau_S$ and a channel $\set{P(y\vert x)}, \tau_C$ Let us say that (source, channel) is compatible if $\forall \varepsilon > 0,\exists Enc, Dec: \overline{P_e} < \varepsilon$.

The theorem we just proved is equivalent to (source, channel) is compatible $\Ra$ $\frac{H}{\tau_S} \leqslant \frac{C}{\tau_C}$.

We now attempt the other direction ($\La$).

Suppose we insist on modular design.

$U_1U_2\ldots$ (stationary, entropy rate $H$)

The channel encoder and channel decoder we seek are of the following kind.
\[
    f:\set{0,1}^k \to \X^n \equiv f:\brackets{1,2^k} \to \X^n\overset{\sim}{\equiv} f:\brackets{1,M} \to \X^n
\]

\begin{definition}
    \[
        \opname{rate}(f) = \frac{k}{n} \equiv \frac{\log M}{n}
    \]
\end{definition}

$\emptyset : \Y^n \to \set{0,1}^k$

\begin{definition}
    \[
        \prob{error\vert i} = \prob{\emptyset(Y_1,\ldots,Y_n) \neq i \vert (X_1\ldots X_n) = f(i))}    
    \]
    \[
        \overline{error}(f,\emptyset,channel) = \frac{1}{M} \sum\limits_{i=1}^M \prob{error\vert i}    
    \]
    \[
        error_{\max}(f,\emptyset,channel) = \max\limits_{1\leqslant i\leqslant M} \prob{error \vert i}
    \]
\end{definition}

\begin{definition}
    Given a channel $\prob{y\vert x}$ we say that $R$ is an achievable rate if $\forall \varepsilon > 0$, there exists $f,\emptyset$ such that $rate(f) \geqslant R$, $error_{\max}(f,\emptyset, P) \leqslant \varepsilon$.
\end{definition}

\begin{theorem}
    Given a channel $\prob{y\vert x}$ with $C= \max_{p_X} I(X,Y)$ then any $R < C$ is achievable.
\end{theorem}

\begin{theorem}[Good news]
    If $\frac{H}{\tau_S} < \frac{C}{\tau_C}$, can achieve $\overline{P_e} \to 0$.
\end{theorem}


\section{The probabilistic method}

$(\exists P_z : \prob{A(z)} > 0)\Ra(\exists z : A(z))$.

$(\exists P_z : \esp{g(z)} \leqslant \alpha)\Ra(\exists z : g(z) \leqslant \alpha)$.

\paragraph{Random coding}

Let $X_i^{(m)}$ be iid $p(x)$.

Let $T^n(\varepsilon) = \set{(x,y) \left\vert \left\lvert \widehat{P}_{\X,\Y}(x,y) - p(x)p(y\vert x) \right\rvert \leqslant \varepsilon p(x)p(y\vert x) \right.}$

\begin{itemize}
    \item Find all $m$ such that $(X^{(m)}, Y) \in T^n(\varepsilon)$
    \item If only one such message $m$, output $m$
    \item Otherwise, output 0.
\end{itemize}


\begin{lemma}
    \begin{itemize}
        \item if $(X,Y) \overset{idd}{\sim} p(x)p(y\vert x)$, then $\prob{(X,Y) \in T^n(\varepsilon)} \to 1$ as $n\to \infty$.
        \item If $(X, Y) \overset{idd}{\sim} p(x)p(y)$ then $\prob{(X,Y) \in T^n(\varepsilon)} \leqslant 2^{-n(I(X,Y)-\delta(\varepsilon))}$, $\delta(\varepsilon) \underset{\varepsilon \to 0}{\longra} 0$
    \end{itemize}
\end{lemma}
\begin{proof}
    \item
    \begin{itemize}
        \item LLN
        \item $D(p\Vert q) = \sum\limits_z p(z) \log\frac{p(z)}{q(z)}$ so $D(P_{XY} \Vert P_Xp_Y) = I(X,Y)$
    \end{itemize}
\end{proof}

\section{Binary Symmetric Channel}

$p(y\vert x) = \begin{cases}
    1-\delta & x = y\\
    \delta & x \neq y
\end{cases}$ with $\X = \Y = \set{0,1}$.


\[
    \begin{aligned}
        H(Y \vert X=0) &= H(Y \vert X=1)\\
        &= h_2(\delta)\\
        &= -\delta \log \delta - (1-\delta) \log (1-\delta)
    \end{aligned}
\]

So

\[
    \begin{aligned}
        I(X,Y) &= H(Y) - H(Y\vert X)\\
        &= H(Y) - h_2(\delta)\\
        &\leqslant 1 - h_2(\delta)
    \end{aligned}
\]

$C = 1-h_2(\delta)$ bits.

\section{Binary Erase Channel}

$\X = \set{0,1}$, $\Y= \set{0,1,e}$.

$p(y\vert x) = \begin{cases}
    1-\varepsilon & y=x\\
    \varepsilon & y=e
\end{cases}$

$e$ for erasure.

\[
    \begin{aligned}
        H(X \vert Y = 0) &= 0
        H(X \vert Y = 1) &= 0
        H(X \vert Y = e) &= H(X)
    \end{aligned}
\]

\[
    \begin{aligned}
        I(X,Y) = (1-\varepsilon) H(X)\\
        &\leqslant 1-\varepsilon
    \end{aligned}
\]

$C = 1-\varepsilon$ bits.

\section{Convexity and Concavity}

$f : D \to \RR$ is convex if blablabla...

Fix $p(y\vert x)$ consider $I(P_x) = I(X,Y) $. 

$D = \set{(P_1,\ldots,P_k) \left\vert \card{\X} = k, \forall i\in\brackets{i,k}, P_i\geqslant 0, \sum\limits_{i=1}^n P_i = 1\right.}$

\begin{theorem}
    $I(P_X)$ est concave.
\end{theorem}

Maximize $f(P_X)$ such that $P(x)\geqslant 0$, $\sum\limits_xP_X(x) = 1$.

$f(P_x) = f(P_1,\ldots,P_k)$. Suppose some $(P_1,\ldots,P_k)$ "believed optimal".

If so, it must be that $f(P_1,\ldots,P_i+\varepsilon,\ldots, P_j-\varepsilon,\ldots,P_k) - f(P_1,\ldots,P_k) \leqslant 0$.

LHS $= \varepsilon \frac{\partial f}{\partial P_i}-\varepsilon\frac{\partial f}{\partial P_j}+o(\varepsilon)$. Should have $\forall(i,j), P_j > 0 \Ra \frac{\partial f}{\partial P_i}-\frac{\partial f}{\partial P_j} \leqslant 0$.In particular $\forall(i,j), P_i>0\wedge P_j > 0 \Ra \frac{\partial f}{\partial P_i}-\frac{\partial f}{\partial P_j} = 0$


\begin{definition}[\textsc{Karush Kuhn Tucker} conditions (KKT)]
    $(P_1,\ldots, P_k)$ satisfies KKT if $\exists \lambda:$
    \[
        \begin{aligned}
            \forall i, P_i>0 &\Ra \frac{\partial f}{\partial P_i} = \lambda\\
            \forall i, P_i=0 &\Ra \frac{\partial f}{\partial P_i} \leqslant \lambda\\
        \end{aligned}    
    \]
\end{definition}

$\approx$ like $\underset{x}{\opname{argmax}} g(x)$ must have $\nabla g(x) = G$ (Unconstrained).

