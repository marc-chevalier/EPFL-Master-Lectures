\section{Definitions}
Output of the source is in discrete time and discrete valued.

\begin{definition}
    The output is a sequence of letters from an (finite) alphabet $\U$.
\end{definition}

We want to decompose the encoder into a source encoder and a channel encoder. So, the number of encoder is linear with respect of the the number of sources and channel and not quadratic.

The source encoder outputs a stream of bits and don't care of the channel and the channel encoder translate the input stream of bits without considering the kind of data.

We can use this architecture without loss of generality.

\bigskip

Source coding : Representation of information sources in bits.

\begin{example}
    $\U = \set{a,b,c,d}$
    \[
        \begin{aligned}
            a &\ra 0\\
            b &\ra 0\\
            c &\ra 0\\
            d &\ra 1
        \end{aligned}
        \qquad \text{bad, "singular"}            
    \]
\end{example}

A source code is a function
\[
    \C : \U \ra \set{0;1}^*
\]

\begin{definition}
    A code $\C$ is called singular  if
    \[
        \exists (u,v) \in \U^2, u \neq v \wedge \C(u) = \C(v)
    \]
\end{definition}

\begin{definition}
    A code $\C$ is called non singular (injective) if
    \[
        \forall (u,v) \in \U^2, u \neq v \Ra \C(u) \neq \C(v)
    \]
\end{definition}

\begin{example}
    \[
        \begin{aligned}
            a &\ra 0\\
            b &\ra 00\\
            c &\ra 1\\
            d &\ra 11
        \end{aligned}
        \qquad \text{still bad, "not uniquely decodable"}            
    \]
\end{example}

\begin{definition}
    Given a code $\C : \U \ra \set{0;1}^*$, define for $n\in\NN^*$
    \[
        \C^n : \U^n \ra \set{0;1}^*
    \]
    as
    \[
        \C^n(u_1\ldots u_n) = \C(u_1) \ldots \C(u_n)
    \]
    and
    \[
        \C^* : \U^* \ra \set{0;1}^*
    \]
    as
    \[
        \C^n(u_1\ldots u_k) = \C(u_1) \ldots \C(u_k)
    \]
\end{definition}

\begin{definition}
    A code $\C$ is said to be uniquely decodable if $\C^*$ is injective (non singular).
\end{definition}

We want our code to be uniquely decodable.

\begin{definition}
    A sequence $u_1\ldots u_m$ is a prefix of $v_1 \ldots v_n$ if $n\leqslant m$ and $\forall i\in\llbracket 1,m\rrbracket, u_i = v_i$.
\end{definition}

\begin{example}
    \[
        \begin{aligned}
            a &\ra 0\\
            b &\ra 10\\
            c &\ra 110\\
            d &\ra 111
        \end{aligned}       
    \]
    
    \[
        \underbrace{110}_{c}\underbrace{110}_{c}\underbrace{0}_{a}\underbrace{111}_{d}\underbrace{10}_{b} \underbrace{0}_{a}
    \]
\end{example}

\begin{definition}
    A code $\C$ is said to be prefix free if $\C(u)$ is not a prefix of $\C(v)$ for all $u\neq v$.
\end{definition}

\begin{theorem}
    A prefix free code is uniquely decodable.
\end{theorem}
\begin{proof}
    Suppose that $\C$ is prefix free but not uniquely decodable. So, there is  $u_1\ldots u_m$ and $v_1\ldots, v_n$ such that $u_1\ldots u_m \neq v_1\ldots, v_n$ and $\C(u_1) \ldots \C(u_m) = \C(v_1) \ldots \C(v_n)$.

    Without loss of generality, we can assume $u_1 \neq v_1$.
    
    Suppose $\C(u_1)$ is longer than $\C(v_1)$ $\Ra$ $\C(v_1)$ is a prefix of $\C(u_1)$: contradiction.
    
    In the other case, $\C(u_1)$ is a prefix of $\C(v_1)$: contradiction.
\end{proof}

\begin{example}
    \[
        \begin{aligned}
            a &\ra 0\\
            b &\ra 01\\
            c &\ra 011\\
            d &\ra 111
        \end{aligned}
        \qquad \text{not prefix free but still uniquely decodable}
    \]
    
    \[
        \underbrace{110}_{c}\underbrace{110}_{c}\underbrace{111}_{d}
    \]
\end{example}

Binary tree representation of source codes.

\begin{center}
	\Tree [.$\emptyset$ [.0 [.00 [000 ] [001 ] ] [.01 [010 ] [011 ] ] ] [.1 [.10 [100 ] [101 ] ] [.11 [110 ] [111 ] ] ] ]
\end{center}

\begin{center}
	\Tree [ [.$\C(b)$ [$\C(a)$ ] [$\C(c)$ ] ] [[ ] [[ ][ ] ] ] ]
	$\ra$ not prefix free.
\end{center}

\begin{center}
    \Tree [[[ ][$\C(a)$ ]][[$\C(b)$ ][[$\C(c)$ ][$\C(d)$ ] ] ] ]
    $\ra$ prefix free
\end{center}

\begin{remark}
    In the binary tree representation of a prefix tree code, all codewords are found on the leaves.
\end{remark}

\section{\textsc{Kraft}'s inequality}

\begin{theorem}[\textsc{Kraft}'s inequality for prefix-free codes]
    If $\C$ is prefix free, then
    \[
        \operatorname{KarftSum}(\C) := \sum\limits_{u\in\U} 2^{-\lvert \C(u) \rvert} \leqslant 1
    \]
\end{theorem}
\begin{proof}
    \[
        \begin{aligned}
            1 &= \sum\limits_{l \in \text{leaves}} \prob{\text{squirrel reaches leaf } l}\\
            &= \sum\limits_{l \in \text{leaves}} 2^{-\operatorname{height}(l)}\\
            &\geqslant \sum\limits_{u\in\U} 2^{-\lvert \C(u)\rvert}
        \end{aligned}
    \]
\end{proof}

\begin{proposition}
    \[
        \opname{KraftSum}(\C^n) = (\opname{KraftSum}(\C))^n
    \]
\end{proposition}
\begin{proof}
    \[
        \begin{aligned}
            \opname{KraftSum}(\C^n) &= \sum\limits_{u_1\ldots u_n\in\U^n} 2^{-\underbrace{\left\lvert\C^n(u_1\ldots u_n)\right\rvert}_{\C(u_1)\ldots\C(u_n)}}\\
            &= \sum\limits_{u_1\ldots u_n\in\U^n} 2^{-\left(\sum\limits_{i=1}^n\left\lvert \C(u_i) \right\rvert\right)}\\
            &= \sum\limits_{u_1\ldots u_n\in\U^n} \prod\limits_{i=1}^n 2^{-\left\lvert\C(u_i) \right\rvert}\\
            &= \prod\limits_{i=1}^n \sum\limits_{u_i}2^{-\left\lvert \C(u_i)\right\rvert}\\
            &= (\opname{KraftSum(\C)})^n
        \end{aligned}
    \]
\end{proof}

\begin{example}
	\begin{center}
	    \begin{tabular}{c|c|c}
	        $u$ & $l(u)$ & $\C(u)$\\\hline
	        $a$ & $1$ & \\
	        $b$ & $2$ & \\
	        $c$ & $3$ & \\
	        $d$ & $3$ & \\
	    \end{tabular}
	\end{center}
	
    Is there a prefix free code $\C$ with $\left\lvert \C(u) \right\rvert = l(u)$.
\end{example}

\begin{theorem}["Reverse" \textsc{Kraft} inequality]
    Given an alphabet $\U$ and a function $l:\U \to \NN$ such that $\sum\limits_{u\in\U} 2^{-l(u)} \leqslant 1$ then there exists a prefix free code $\C : \U \to \set{0,1}^*$ such that $\left\lvert \C(u)\right\rvert = l(u)$ for each $u\in\U$.
\end{theorem}
\begin{proof}
    Suppose the alphabet $\U$ has $k$ letters, assume $\U = \llbracket 1;k \rrbracket$. Suppose also without loss of generality thaht $\forall i \in \llbracket 1; k-1 \rrbracket, l(i) \leqslant l(i+1)$.
    
    Consider the following algorithm:

    \begin{enumerate}
        \item Start with the binary tree of height $l(k)$ with all nodes unoccupied.
        \item For $i = 1,\ldots,k$: place $\C(i)$ at an unoccupied node at height $l(i)$ and mark as occupied all nodes that descend from it.
        \item Return $\C$.
    \end{enumerate}
    
    When we try to find a free node in the tree, how many unoccupied nodes are there at height $l(i)$?
    
    \[
        \begin{aligned}
            &= \underbrace{2^{l(i)}}_{\text{we start with}} - \underbrace{2^{l(i)-l(1)}}_{\substack{\text{nodes eliminated}\\ \text{when placing }\C(1)}} - \cdots - \underbrace{2^{l(i)-l(i-1)}}_{\substack{\text{nodes eliminated}\\ \text{when placing }\C(i-1)}}\\
            &= 2^{l(i)} \left( 1-\underbrace{\left(\sum\limits_{k=1}^{i-1} 2^{-l(k)} \right)}_{< 1} \right)\\
            &> 0
        \end{aligned}
    \]
\end{proof}

\begin{proposition}[\textsc{Kraft}'s inequality for non-singular codes]
    Suppose $\C : \U \to \set{0,1}^*$ is a non singular code then
    \[
        \begin{aligned}
            \opname{KraftSum}(\C) &= \sum\limits_{u\in\U} 2^{-\left\lvert \C(u) \right\rvert}\\
            &\leqslant 1+ \max\limits_n \left\lvert \C(u)\right\rvert
        \end{aligned}
    \]
\end{proposition}
\begin{proof}
    Let $l_{max}$ the height of the tree.
    
	    \Tree [  [  [  [ $\vdots$ ] [ $\vdots$ ] ] [  [ $\vdots$ ] [ $\vdots$ ] ] ] [  [  [ $\vdots$ ] [ $\vdots$ ] ] [  [ $\vdots$ ] [ $\vdots$ ] ] ] ]

    Height 0: $2^{-0} = 1$
    
    Height 1: $2^1 2^{-1} = 1$
    
    Height 2: $2^2 2^{-2} = 1$
\end{proof}

\begin{theorem}
    If $\C$ is a uniquely decodable code then $\opname{KraftSum}(\C) \leqslant 1$.
\end{theorem}
\begin{proof}
    Suppose $\C$ is uniquely decodable, in particular $\C^n$ is non singular.
    \[
        \begin{aligned}
            \left (\opname{KraftSum}(\C)\right )^n &= \opname{KraftSum}(\C^n) \leqslant 1+\max\limits_{u_1\ldots u_n} \left \lvert \C(u_1)\ldots\C(u_n) \right \rvert\\
            &= 1 + n \max\limits_u \left \lvert \C(u) \right \rvert\\
            &= \cplx{n}
        \end{aligned}
    \]
    
    If $\opname{KraftSum}(\C)>1$ would be $\cplx{\exp(n)} \Ra \opname{KraftSum}(\C)\leqslant 1$.
\end{proof}
\begin{corollary}
    If $\C$ is a uniquely decodable code, then there exists a prefix free code $\C'$ such that
    \[
        \forall u \in \U, \left\lvert \C'(u) \right\rvert = \left\lvert \C(u) \right\rvert    
    \]
\end{corollary}

\section{Entropy}

\begin{definition}
    Expected codeword length
    \[
        \sum\limits_i \prob{a_i} \left\lvert \C(a_i) \right\rvert
    \]
\end{definition}


\begin{definition}[Entropy]
    For a discrete random variable $X$ which has a mass function $P$
    \[
        H(X) := -\sum\limits_i P{x_i}\log(P(x_i))
    \]
\end{definition}

\begin{theorem}
   Expected codeword length $\geqslant$ Entropy
\end{theorem}

\begin{example}
    $\U = \set{a,b,c,d}$.
    \[
        \begin{array}{cccc}
            \prob{a} = \frac{1}{2} &
            \prob{b} = \frac{1}{4} &
            \prob{c} = \frac{1}{8} &
            \prob{d} = \frac{1}{8}
        \end{array}
    \]
    
    \[
        \begin{aligned}
            a &\ra 0\\
            b &\ra 10\\
            c &\ra 110\\
            d &\ra 111
        \end{aligned}
    \]
    
    \[
        \esp{\left\lvert \C(\U) \right\rvert} = 1.75 = H(\U)
    \]
\end{example}

\begin{theorem}
    Given a source $\U$, there exists a prefix code $\C$ such that
    
    \[
        \esp{\left\lvert \C(\U) \right\rvert}\leqslant H(\U) + 1
    \]
\end{theorem}
\begin{proof}
    Take $l(u) = \ceil{\log_2\frac{1}{p(u)}}$

    \[
        \log \frac{1}{p(u)} \leqslant l(u) \leqslant 1 + \log \frac{1}{p(u)}
    \]
    
    Using the left inequality.
    
    \[
        \begin{aligned}
            2^{-l(u)} \leqslant p(u) &\Ra \sum\limits_u 2^{-l(u)} \leqslant \sum\limits p(u) = 1\\
            &\Ra \exists \text{a prefix free }\C:\left\lvert \C(u)\right\rvert = l(u)
        \end{aligned}            
    \]    
    
    
    Also
    \[
        \begin{aligned}
            \esp{\left\lvert \C(\U) \right\rvert} &= \sum\limits_u p(u)l(u)\\
            &\leqslant \sum\limits_u p(u) \left( 1+\log\frac{1}{p(u)}\right)\\
            &=1+H(\U)
        \end{aligned}
    \]
\end{proof}


\begin{proposition}
    \[
        H(\U) \geqslant 0    
    \]
\end{proposition}
\begin{proof}
    \[
        \begin{aligned}
            H(U) &= \sum \underbrace{p(u)}_{\geqslant 0} \overbrace{\log \underbrace{\frac{1}{p(u)}}_{\geqslant 1}}^{\geqslant 0}\\
            &\geqslant 0
        \end{aligned}            
    \]
\end{proof}

\begin{proposition}
    \[
        H(\U) \leqslant \log \left\lvert \U \right\rvert
    \]
\end{proposition}
\begin{proof}
    \[
        \begin{aligned}
            \sum p(u) \log \frac{1}{p(u)} - \sum p(u) \log\left\lvert \U \right\rvert &= \sum p(u) \log \frac{1}{p(u) \left\lvert\U\right\rvert}\\
            &= \left( \sum p(u) \log \frac{1}{p(u)\left\lvert \U \right\rvert} \right) \left( \log e\right)\\
            &\leqslant \left( \log e\right)\left( \sum p(u)\left( \frac{1}{p(u)\left\lvert \U\right\rvert}-1\right) \right)\\
            &=\left( \log e\right)\left( 1-1 \right)\\
            &= 0
        \end{aligned}
    \]
\end{proof}

\begin{proposition}
    Suppose $U$ and $V$ are independent random variables. Then 
    \[
        H(UV) = H(U) + H(V)
    \]
\end{proposition}
\begin{proof}
    We can consider $(U,V)$ as a new random variable.
    
    \[
        \prob{(U,V) = (u,v)} = \frac{\prob{U = u} \prob{V = v}}{p(u,v)}
    \]
    
    Then 
    \[
        \begin{aligned}
            H(UV) &= \sum p(uv) \log \frac{1}{p(uv)}\\
            &= \sum p(uv) \log \frac{1}{p(u)p(v)}\\
            &= \sum p(uv) \left( \log \frac{1}{p(u)} + \log \frac{1}{p(v)}\right)\\
            &= \sum\limits_{u,v} p(uv) \log \frac{1}{p(u)} + \sum\limits_{u,v} p(uv) \log \frac{1}{p(v)}\\
            &= \sum\limits_{u} p(u) \log \frac{1}{p(u)} + \sum\limits_{v} p(v) \log \frac{1}{p(v)}\\
        \end{aligned}
    \]
\end{proof}

Suppose we have a memoryless, stationary source, producing $\U_1$, $\U_2$, $U_3$, $\U_4$...

So fare, we choose to represent the source output in a letter to letter fashion. If instead, we use a code to represent $n$ letters at a time, we will have
\[
    H(\U_1\ldots \U_n) \leqslant \frac{\esp{\left\lvert \C(\U_1\ldots \U_n)\right\rvert}}{n} \leqslant \frac{H(\U_1\ldots \U_n) +1}{n}
\]

Also
\[
    \begin{aligned}
        \frac{1}{n}H(\U_1\ldots \U_n) &= \frac{1}{n} \sum\limits_{i=1}^n H(\U_i)\\
        &= \frac{1}{n}\sum\limits_{i=1}^n H(\U_1)\\
        &= H(\U_1)
    \end{aligned}
\]

So far we have seen bounds to the performance of code design, but we have not seen how to actually design a prefix code.

Given $p:\U \to \RR$. 

$\min\limits_{l:\U\to\NN} \sum p(u) l(u)$ 

$\sum\limits_u 2^{-l(u)} \leqslant 1$

Properties of optimal (in terms of minimizing $\sum p(u) \lvert\C(u)\rvert$ codes

\begin{enumerate}
    \item if $p(u) < p(v)$ then $l(u) \geqslant l(v)$.
        \begin{proof}
            Suppose not $p(u) < p(v)$ but $l(u) < l(v)$. Then swap the codewords for $u$ \& $v$. That improve the code.
        \end{proof}
        
    \item In an optimal prefix code, there are at least 2 longest codewords.
        \begin{proof}
            If not, the longest codeword can be shortened without violating the prefix free condition.
        \end{proof}
        
    \item Among optimal codes, there is one for which the two least probable symbols are siblings.
        \begin{proof}
            They have to be at the same height. We just need to permute in order to make the two least probable symbols siblings.
        \end{proof}
        
    \item 1-to-1 correspondence between prefix free codes for an alphabet $\U$ and guessing strategies for $\U$.
        \[
            \esp{\#\text{ of question to guess}} = \esp{\lvert \text{codeword} \rvert}
        \]
\end{enumerate}

Suppose $U$, $V$ are random variables. Suppose we know $V$ ($V = v$). How much entropy is in $\U$?

We know that under the conditioning $V=v$, the probability that $U=u$ is now $\prob{U=u=\vert V=v} = \frac{p(uv)}{p(v)} =: p(u\vert v)$. So we define

\[
    \begin{aligned}
        H(U\vert V=v) &= \sum_u p(u\vert v) \log \frac{1}{p(u\vert v)}\\
        H(U\vert V) &= \sum_v p(v) H(U\vert V=v)\\
    \end{aligned}
\]

\begin{conjecture}
    \[
        H(U\vert V) \leqslant H(U)
    \]
\end{conjecture}

\begin{conjecture}
    \[
        H(UV) = H(U) + H(V \vert U) = H(V) + H(U \vert V)
    \]
\end{conjecture}

\begin{example}
    $U$, $V$ are binary random variables. $\U = \set{a,b}$, $\V = \set{0,1}$
    
    \begin{tabular}{c|cc}
        $p(uv)$ & $0$ & $1$\\
        \hline
        a & $\frac{1}{2}$ & $\frac{1}{4}$\\
        b & $0$ & $\frac{1}{4}$
    \end{tabular}
    
    \[
        \begin{aligned}
            H(UV) &= \frac{1}{2} \log 2 + \frac{1}{4} \log 4 + \frac{1}{4} \log 4\\
            &= 1.5
        \end{aligned}
    \]

    \[
        \begin{array}{lll}
            H(V) = 1 & H(U\vert V=0) = 0 & H(V\vert U=b) = 0 \\
            H(U) = \frac{3}{4} \log\frac{4}{3} + \frac{1}{4}\log 4 & H(U\vert V=1) = 1 & H(V\vert U=a) = 1 \\
            & H(U\vert V) = \frac{1}{2} & H(V\vert U) = \frac{3}{4}\left( \frac{3}{4} \log\frac{4}{3} + \frac{1}{4}\log 4 \right)
        \end{array}        
    \]
\end{example}

\begin{proof}
    \[
        \begin{aligned}
           p(uv) &= p(u) p(v\vert u)\\
           \Ra \log \frac{1}{p(uv)} &= \log \frac{1}{p(u)} + \log \frac{1}{p(u\vert v)}\\
           H(UV) &= \esp{\log \frac{1}{p(UV)}}\\
           &= \underbrace{\esp{\log\frac{1}{p(U)}}}_{H(U)} + \underbrace{\esp{\log\frac{1}{p(V\vert U)}}}_{H(V\vert U)}
        \end{aligned}            
    \]

    \[
        \begin{aligned}
            H(V\vert U) &= \sum_u p(u) H(V\vert U=u)\\
            &= \sum_u p(u) \sum_v p(v\vert u) \log \frac{1}{p(v\vert u)}\\
            &= \sum_{v,u} p(vu) \log \frac{1}{p(v\vert u)}
        \end{aligned}
    \]
\end{proof}

\begin{definition}
\[
    \begin{aligned}
        H(U) + H(V) - H(UV) &= H(V) - H(V\vert U)\\
        &= H(U) - H(U\vert V)
    \end{aligned}
\]

This difference is the mutual information $I(U,V)$ between $U$ and $V$, and represent the reduction of the effort to guess $U$ by knowing $V$ (and vice-versa by symmetry).
\end{definition}

\begin{lemma}
    Suppose $\W$ an alphabet and $p$ and $q$ are two probability distribution on $\W$, then
    \[
        \sum\limits_w p(w) \log \frac{p(w)}{q(w)}\geqslant 0
    \]
\end{lemma}
\begin{proof}
    We need to prove $\sum p(w) \log \frac{q(w)}{p(w)} \leqslant 0$.
    
    \[
        \begin{aligned}
            \sum p(w) \log \frac{q(w)}{p(w)} &\leqslant \sum p(w) (\frac{q(w)}{p(w)}-1)\\
            &= \sum q(w) - p(w)\\
            &= 1-1\\
            &= 0
        \end{aligned}            
    \]
\end{proof}

\begin{theorem}
    \[
        I(U,V) \geqslant 0    
    \]
\end{theorem}
\begin{proof}
    \[
        \begin{aligned}
            I(U,V) &= H(U) + H(V) - H(U,V)\\
            &= \esp{\log\frac{p(UV)}{p(U)p(V)}}\\
            &= \sum\limits_{u,v} p(uv) \frac{p(uv)}{p(u)p(v)}\\
          &\geqslant 0  
        \end{aligned}            
    \]
\end{proof}

\begin{theorem}
    \[
        H(U_1\ldots U_n) = H(U_1) + H(U_2\vert U_1) + H(U_3\vert U_1 U_2) + \cdots + H(U_n \vert U_1\ldots U_{n-1})
    \]
\end{theorem}
\begin{proof}
    \[
        \begin{aligned}
            p(u_1\ldots u_n) &= p(u_1) p(u_2 \vert u_1) p(u_3\vert u_1u_2) \cdots p(u_n \vert u_1\ldots u_1)\\
            \Ra \log\frac{1}{p(u_1\ldots u_n)} &= \log\frac{1}{p(u_1)} +  \log\frac{1}{p(u_2 \vert u_1)}+ \log\frac{1}{p(u_3\vert u_1u_2)} + \cdots + \log\frac{1}{p(u_n \vert u_1\ldots u_1)}
        \end{aligned}
    \]
\end{proof}

\begin{definition}[Condition mutual information]
    \[
        \begin{aligned}
            I(U,V \vert W) &= H(U\vert W) + H(V\vert W) - H(UV \vert W)\\
             &= H(U \vert W) - H(U \vert V W)\\
             &= H(V \vert W) - H(V \vert U W)
        \end{aligned}
    \]
\end{definition}

\begin{theorem}
    \[
        I(U,V \vert W) \geqslant 0
    \]
    with $=$ when $U$ and $V$ are independent conditional in $W$.
\end{theorem}

\begin{theorem}
    \[
        I(U_1\ldots U_n,V) = I(U_1,V) + I(U_2,V\vert U_1    ) + I(U_3,V\vert U_1, U_2) + \cdots + I(U_n, V \vert U_1\ldots U_n)
    \]
\end{theorem}
\begin{proof}
    \[
        I(U_1\ldots U_n, V) = H(U_1\ldots U_n) - H(U_1\ldots U_n \vert V)    
    \]
    
    \[
        \begin{aligned}
            H(U_1\ldots U_n) &= H(U_1) + H(U_2\vert U_1) + \cdots + H(U_n \vert U_1 \ldots U_{n-1})\\
            H(U_1\ldots U_n \vert V) &= H(U_1 \vert V) + H(U_2\vert U_1 V) + \cdots + H(U_n \vert U_1 \ldots U_{n-1} V)\\
            I(U_1\ldots U_n, V) &= I(U_1,V) + I(U_2 V \vert U_1) + \cdots + I(U_n V \vert U_1 \ldots U_{n-1})
        \end{aligned}
    \]
\end{proof}

\begin{remark}[\textsc{Markov} chains]
    Suppose $X$, $Y$ and $Z$ are random variables.
    
    It is always true that
    \[
        p(x,y,z) = p(x)p(y\vert x) p(z\vert x,y)
    \]
    
    If $X\-- Y\-- Z$, then
    \[
        p(x,y,z) = p(x)p(y\vert x) p(z\vert y)
    \]
\end{remark}