So far we have seen codes of the type
\[
    \U^n \to \set{0,1}^*
\]

They are called Fixed-to-variable-length codes and all have error free recovery of the source from its representation.

We now discuss "Fixed-to-fixed" codes, we want
\[
    \C: \U^n \to \set{0,1}^k
\]

\begin{example}
    Suppose $\U=\set{a,b,c,d}$.
    \[
        \C: \U^n \to \set{0,1}^k
    \]
    $\card{\U^n} = 4^n$ and 
    $\card{\set{0,1}^k} = 2^k$
    
    error free recover requires $k\geqslant 2n$.
\end{example}

To obtain efficient codes, we will give up error-free recovery replace this by recovery with very small probability of error. For this, we will have the code assign binary representation only to a subset $\S \subseteq \U^n$. We will choose this subset to ensure 
\[
    \prob{(U_1\ldots U_n)\in\S} \simeq 1 \wedge \card{\S} \leqslant 2^k
\]

\begin{example}
    Suppose $\U = \set{a,b}p$, $p_a = 0.25$, $p_b = 0.75$. Source is memoryless stationary (iid) $U_1, U_2, \ldots$
    
    What kind of sequences $(u_1,\ldots,u_n)$ do we expect the source to produce?
    
    We expect that $(u_1,\ldots,u_n)$ to have $\simeq \frac{1}{4} n$ a's and $\simeq \frac{3}{4} n$ b's.
    
    Pick $\varepsilon > 0$ small. Let us define $\S = \set{\left.(u_1,\ldots,u_n) \middle\vert \length{(u_1,\ldots,u_n)}_a = \frac{1}{4}n(1\pm \varepsilon)\right.}$ .
    
    Intuitively, we expect $\set{(u_1,\ldots,u_n) \in \S}$ to be a high probability event.
    
    How about the size of $\S$?
    
    How many sequences $(u_1,\ldots,u_n)$ are there with $i$ a's and $n-i$ b's? 
    \[
        \binom{n}{i}
    \]
    So
    \[
        \begin{aligned}
            \card{\S} &= \sum\limits_{i = \ceil{\frac{n}{4}(1-\varepsilon)}}^{\floor{\frac{n}{4}(1+\varepsilon)}} \binom{n}{i}\\
            &\leqslant (n+1)\binom{n}{\floor{\frac{n}{4}(1+\varepsilon)}}
        \end{aligned}
    \]
    \[
        \begin{aligned}
            1 &= (p + (1-p))^n\\
            &=\sum\limits_{i=0}^n \binom{n}{i} p^i (1-p)^{n-i}\\
            &\geqslant \binom{n}{i} p^i (1-p)^{n-i}\\
            \Ra \binom{n}{i} &\leqslant \left(\frac{1}{p}\right)^i\left(\frac{1}{1-p}\right)^{n-i}\\
            &\leqslant \left(\frac{n}{i}\right)^i\left(\frac{n}{n-i}\right)^{n-i} \qquad \text{ with } p=\frac{i}{n}
        \end{aligned}
    \]
    
    \[
        \log\binom{n}{i} \leqslant n\left( \alpha \log\frac{1}{\alpha} + (1-\alpha)\log\frac{1}{1-\alpha} \right)
    \]
    \[
        \binom{n}{\floor{\frac{n}{4}(1+\varepsilon)}} \leqslant 2^{nH(U')}
    \]
    where $U' = \begin{cases}
        a & \text{ wp } \frac{1}{4}(1+\varepsilon)\\
        b & \text{ wp } 1 - \frac{1}{4}(1+\varepsilon)
    \end{cases}$.
    
    So $\card{\S} \leqslant (n+1)2^{nH(U')}$ so if we choose $k = aH(U') + \log(n+1)$ we will have $\card{} \leqslant 2^k$ with $\frac{k}{n} = \underbrace{H(U') + \frac{\log(n+1)}{n}}_{\simeq H(u) \text{if }\varepsilon\text{ is small and }n\text{ is large}}$ we will have "almost free" representation.
\end{example}

\subsection{General case}

We are given an alphabet $\U$ and a distribution $p$ on $\U$, $\set{p(u) \vert u\in\U}$, and we have a source that produces iid letters $U_1,U_2,\ldots$ each with distribution $p$. We want to have a set $T_{n,\varepsilon, p} \subseteq \U^n$ with the properties
\begin{enumerate}
    \item $\prob{(U_1\ldots U_n) \in T_{n,\varepsilon,p}} \simeq 1$
    \item $\card{T_{n,\varepsilon,p}} \leqslant ...$
\end{enumerate}

We say a sequence $(u_1\ldots u_n) \in \U^n$ to be $\varepsilon$-typical with respect to the distribution $p$ if for every $u\in\U, \frac{\card{\set{i \vert u_i = u}}}{n} = p(u) (1\pm \varepsilon)$.

Set $T_{n,\varepsilon, p}$ to be set of $(u_1,\ldots u_n)$'s which are $\varepsilon$-typical wrt $p$.

\begin{proposition}
    \[
        \prob{(U_1\ldots U_n) \not\in T_{n,\varepsilon,p}} \leqslant \frac{1}{n \varepsilon^2p_{\min}}
    \]
\end{proposition}
\begin{proof}
    Let $B_u$ be the vent that $\frac{1}{n}\length((U_1\ldots U_n))_u \not\in [p(u)(1-\varepsilon); p(u)(1+\varepsilon)]$. Then 
    \[
        \begin{aligned}
            \prob{(U_1\ldots U_n) \not\in T_{n,\varepsilon,p}} &= \prob{\bigcup\limits_{u\in\U} B_u}\\
            &\leqslant \sum\limits_{u\in\U} \prob{B_u}
        \end{aligned}
    \]
    
    $B_u$ is the event that $\left\lvert \frac{1}{n} \sum\limits_{i=1}^n \underbrace{\mathbb{1}\set{U_i = n}}_{X_i = \begin{cases}
    1 & \text{ wp } p(u)\\
    0 & \text{ wp } 1-p(u)
    \end{cases}} - p(u) \right\rvert > \varepsilon p(u)$.
    
    ie. $\left\lvert \frac{1}{n} \sum\limits_{i=1}^n X_i - p(u) \right\rvert > \varepsilon p(u)$.
    
    \[
        \prob{B_u} \leqslant \frac{1}{n\varepsilon^2 p_{\min}}
    \]
    where $p_{\min} = \min p(u)$.
\end{proof}

For the size of $T_{n, \varepsilon, p}$ observe first the followings
\begin{itemize}
    \item Fix a $(u_1,\ldots,u_n)$ in $T_{n,\varepsilon, p}$. Note that
    \[
        \begin{aligned}
            \prob{(u_1 \ldots u_n)=(U_1 \ldots U_n)} &= p(u_1)\cdots p(u_n)\\
            &= \prod\limits_{u\in\U}p(u)^{\overbrace{\length{u_1\ldots u_n}_u}^{np(n)(1\pm\varepsilon)}} \geqslant \prod\limits_u p(u)^{np(u)(1+\varepsilon)}\\
            &= \prod\limits_u 2^{np(u)\log p(u)(1+\varepsilon)} = 2^{-n(1+\varepsilon)H(u)}\\
            \Ra 1 &\geqslant \prob{(U_1\ldots U_n)\in T_{n,\varepsilon,p}}\\
            &= \sum\limits_{(u_1\ldots u_n \in T_{n,\varepsilon,p})} \prob{(U_1\ldots U_n) = (u_1\ldots u_n)}\\
            & \geqslant 2^{-n(1+\varepsilon)H(u)}\card{T_{n,\varepsilon,p}}\\
            \Ra \card{T_{n,\varepsilon,p}} &\leqslant 2 ^{(1+\varepsilon)nH(U)}
        \end{aligned}
    \]
\end{itemize}

\begin{theorem}
    If $U_1\ldots U_n$ are iid $\sim p$, then
    \[
        \forall \varepsilon > 0, \lim\limits_{n\to\infty}\prob{(U_1 \ldots U_n) \in T_{n, \varepsilon, p}} = 1
    \]
    
    Furthermore if $(u_1 \ldots u_n)\in T_{n,\varepsilon, p}$, then
    \[
        \begin{aligned}
            \prob{(U_1 \ldots U_n) = (u_1 \ldots u_n)} &= \prod\limits_{i=1}^n p(u_i)\\
            &= \prod\limits_{u\in\U} p(u) ^{\overbrace{\length{u_i}_u}^{np(n)(1\pm\varepsilon)}}\\
            &= 2^{-nH(u)(1\pm\varepsilon)}
        \end{aligned}
    \]
    ie.
    \[
        2^{-nH(u)(1+\varepsilon)} \leqslant \prob{(U_1 \ldots U_n) = (u_1 \ldots u_n)} \leqslant 2^{-nH(u)(1-\varepsilon)}
    \]
    Also $\card{T_{n,\varepsilon,p}} \leqslant 2^{nH(u)(1+\varepsilon)}$ and $\card{T_{n,\varepsilon,p}} \geqslant (1-\varepsilon) 2^{nH(u)(1-\varepsilon)}$
\end{theorem}

\begin{theorem}
    If $U_1\ldots U_n$ iid $\sim q$ and $(u_1\ldots u_n)\in T_{n,\varepsilon,p}$, then
    \[
        \begin{aligned}
            \prob{(U_1\ldots U_n) = (u_1\ldots u_n)} &= \prod\limits_{i=1}^n q(u_i)\\
            &= \prod\limits_{u\in\U} q(u)^{np(u)(1\pm \varepsilon)}\\
            &= 2^{-n(1\pm\varepsilon)\sum\limits_u p(u) \log\frac{1}{q(u)}}
        \end{aligned}
    \]
    But
    \[
        \begin{aligned}
            \sum\limits_u p(u) \log\frac{1}{q(u)} &= \sum\limits_u p(u) \log\frac{1}{p(u)} + \sum\limits_u p(u) \log\frac{p(u)}{q(u)}\\
            &= H(p) + \underbrace{D(p \Vert q)}_{\text{divergence}}
        \end{aligned}
    \]
    So
    \[
        \prob{(U_1\ldots U_n) = (u_1\ldots u_n)} = 2^{-(1\pm\varepsilon)n(D(q\Vert p + H(p))}
    \]
\end{theorem}

\begin{corollary}
    If $U_1\ldots U_n$ iid $\sim q$
    \[
        \begin{aligned}
            \prob{(U_1\ldots U_n)\in T_{n,\varepsilon,p}} &= \underbrace{2^{-(1\pm\varepsilon)n(H(p) + D(q\Vert p))}}_{\substack{\text{upper and lower bound on}\\\prob{(U_1\ldots U_n) = (u_1\ldots u_n)}}} \cdot \underbrace{(1\pm \varepsilon) 2 ^{nH(p)(1\pm \varepsilon)}}_{\substack{\text{upper and lower}\\\text{bound on }\card{T}}}\\
            &= 2^{-n(1\pm \varepsilon) D(p\Vert q)}2^{\pm n\varepsilon H(p)} (1\pm\varepsilon) \\
            &\doteq 2^{-nD(p\Vert q)}
        \end{aligned}
    \]
\end{corollary}


Recall also
\[
    \begin{aligned}
        I(X,Y) &= \sum\limits_{x,y} p(x,y) \log\frac{p(x,y)}{p(x)p(y)}\\
        &= D(p_{XY} \Vert p_X p_Y)
    \end{aligned}
\]

Consequently if $(X_1,Y_1)\ldots(X_n,Y_n)$ iid $p_Xp_Y$ then
\[
    \prob{((X_1,Y_1)\ldots(X_n,Y_n)) \in T_{n,\varepsilon, p_{XY}}} \doteq 2^{-n\overbrace{I(X;Y)}^{\substack{\text{computed}\\\text{wrt. } p_{XY}}}}
\]

\bigskip

"Application" of typical sequences in source coding. Given iid source $U_1U2\ldots$ $\sim p$. Pick $\varepsilon > 0$ (small) and $n$ (long) so that $T_{n,\varepsilon, p}$ has large probability ($>1-\varepsilon$). Pick $k = (1+\varepsilon)nH(u)$ so that $2^k$ is $\geqslant \card{T_{n,\varepsilon, p}}$. Consider the following source coding method.

First, assign to every member of $T_{n,\varepsilon,p}$ a distinct element of $\set{0,1}^k$, call this $\C : T_{n,\varepsilon,p} \to \set{0,1}^k$.

The source code is the following: if $(U_1\ldots U_n)\in T_{n,\varepsilon,p}$, represent it by $\C(U_1\ldots U_n)\in\set{0,1}^k$ else destroy the universe (it seems there is a nuclear button to do that).

This method uses $(1+\varepsilon) H(u) + \frac{1}{n}$ bits/letter and has a probability of decoding failure $< \varepsilon$.


\bigskip

"Dual" of Fix-to-variable source coding $\equiv$ variable to fixed length source coding (dictionary based source coding).

The idea: given an alphabet $\U$, we will find a dictionary $\D \subsetneq \U^*$, we will assign $\ceil{\log\card{\D}}$ bits binary representation to words in $\D$, wand then given $U_1U_2\ldots$ we will parse it into $w_1w_2\ldots$ of dictionary words, and represent each word by its binary description.

\begin{example}\label{codes_fixed_to_fixed:ex:1}
    $\U = \set{a,b,c}$, $\D = \set{\underbrace{a}_{000},\underbrace{ba}_{001},bb,bca,bcb,bcc,\underbrace{c}_{110}}$ requires no lookahead.
    
    \[
        bcaababccab\ldots \to \underbrace{bca}_{011} \underbrace{a}_{000} \underbrace{ba}_{001}\underbrace{bcc}_{101}\underbrace{a}_{000}
    \]
\end{example}

\begin{example}\label{codes_fixed_to_fixed:ex:2}
    $\U = \set{a,b,c}$, $\D=\set{aa,ab,b,c}$ $\ra$ invalid because it cannot parse $ac$.
\end{example}

\begin{example}\label{codes_fixed_to_fixed:ex:3}
    $\U = \set{a,b,c}$, $\D=\set{a,aa,aaa,b,c}$ requires lookahead.
    \[
        aabcab \to aa,b,c,a,b    
    \]
    first word as long as possible.
\end{example}

Tree representation of dictionaries.

Example \ref{codes_fixed_to_fixed:ex:1}:
\Tree [ [ $a$ ] [ [ $ab$ ][ $bb$ ][ [ $bca$ ][ $bcb$ ][ $bcc$ ] ] ] [ $c$ ] ]

Example \ref{codes_fixed_to_fixed:ex:2}:
\Tree [ [ [ $aa$ ] [ $ab$ ] ] [ $b$ ] [ $c$ ] ]

Example \ref{codes_fixed_to_fixed:ex:3}:
\Tree [ [ .$a$ [ .$aa$ [ .$aaa$ ] [ ] [ ] ] [ ] [ ] ] [] [ $c$ ] ]

The words of a valid \& no-lookahead (=prefix free) dictionaries form the leaves of a complete $\card{\U}$-ary tree.

Let us examine the working of a valid \& no-lookahead prefix-free dictionary.

\[
    \underbrace{U_1\ldots U_n}_{\text{iid.}} \to \underbrace{W_1}_{\in \D} \ldots W_m \ra \text{binary representation in }\ceil{\log\card{\D}}
\]

$W's$ are also iid.

\# of bits/letters $=\frac{m\log\ceil{\card{\D}}}{\sum\limits_{i=1}^n\length{W_i}} \to \frac{\ceil{\card{\D}}}{\esp{\length{W_1}}}$.