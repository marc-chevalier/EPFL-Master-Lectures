Template:
\begin{enumerate}
    \item $i=1$
    \item Starting point $x_i$
    \item Compute $\nabla f(x_i)$.
    \item Either $\nabla f(x_i) = 0$ (OK) or figure out $x_{i+1}$ goto 2.
\end{enumerate}

We don't expect to solve the convex program exactly.

Given $\varepsilon > 0$, the goal is to output a point $x$ such that
\[
    \begin{aligned}
	    f(x^*) &\leqslant f(x)  + \varepsilon\\
	    \lvert f(x)-f(x^*)\rvert &\leqslant \varepsilon
    \end{aligned}
\]

The running time depend of $\frac{1}{\varepsilon}$.


Start with $x_1$.

\[
    x_{t+1} = x_t - \underbrace{\eta_t}_{\substack{>0\\\text{step size}}} \nabla f(x_t)
\]

Why is this algorithm good?


\begin{theorem}
    If $f$ is convex, differentiable, $\nabla f$ is $L$-Lipschitz start at $x_1$ such that $\left\lVert x_1-x^* \right\rVert \leqslant D$, is we pick $T = o\left( \frac{LD^2}{\varepsilon} \right)$, $f(x_T) \leqslant f(x^*) + \varepsilon$.
\end{theorem}

We are allows to change $f$ at time $t$. $f_t$ is convex and $\forall t, \forall x, \lVert \nabla f_t(x) \rVert \leqslant G$.

\begin{corollary}
    If $f_1, f_2,\ldots$, convex differentiable function, $\lVert \nabla f_t\rVert \leqslant G$, then given $\varepsilon > 0$, "good" $x_1$, then after $T \approx \left( \frac{G}{\varepsilon} \right)^2$ iterations, 
    \[
        \underbrace{\frac{1}{T} \sum\limits_{t=1}^T f_t(x_t)}_{\text{algo output}} \leqslant \underbrace{\inf\limits_x \sum_{t=1}^T f_t(x)}_{\text{Best possible}} + \varepsilon
    \]
\end{corollary}

Online convex optimization.


    $\lVert \nabla f \rVert_2 \leqslant \sqrt{n}\lVert \nabla f\rVert_\infty$ So we can use $\lVert\quad\rVert_\infty$\footnote{En dimension finie, toutes les normes sont Ã©quivalentes}.


